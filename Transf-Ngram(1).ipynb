{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06bb002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "=== Teste de RAM ===\n",
      "RAM Base (MB): 12.82\n",
      "RAM Com Engram (MB): 16.10\n",
      "\n",
      "=== Tempo vs seq_len ===\n",
      "\n",
      "seq_len=16\n",
      "  Base:   0.000853s\n",
      "  Engram: 0.001010s\n",
      "\n",
      "seq_len=32\n",
      "  Base:   0.000851s\n",
      "  Engram: 0.001018s\n",
      "\n",
      "seq_len=64\n",
      "  Base:   0.000850s\n",
      "  Engram: 0.001015s\n",
      "\n",
      "seq_len=128\n",
      "  Base:   0.000853s\n",
      "  Engram: 0.001019s\n",
      "\n",
      "=== Complexidade TeÃ³rica ===\n",
      "\n",
      "seq_len=16\n",
      "  Base complexity:   32768\n",
      "  Engram complexity: 327680\n",
      "\n",
      "seq_len=32\n",
      "  Base complexity:   131072\n",
      "  Engram complexity: 720896\n",
      "\n",
      "seq_len=64\n",
      "  Base complexity:   524288\n",
      "  Engram complexity: 1703936\n",
      "\n",
      "seq_len=128\n",
      "  Base complexity:   2097152\n",
      "  Engram complexity: 4456448\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import tracemalloc\n",
    "import math\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# FastEngram (CORRIGIDO)\n",
    "# ==========================================================\n",
    "\n",
    "class FastEngram(nn.Module):\n",
    "    def __init__(self, memory_size, dim, proj_dim=64, k=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, dim) * 0.02)\n",
    "        self.proj = nn.Linear(dim, proj_dim, bias=False)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, N, D]\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # ProjeÃ§Ã£o reduzida\n",
    "        x_proj = self.proj(x)               # [B, N, d']\n",
    "        mem_proj = self.proj(self.memory)   # [M, d']\n",
    "\n",
    "        # Similaridade\n",
    "        scores = torch.matmul(x_proj, mem_proj.T)  # [B, N, M]\n",
    "\n",
    "        # Top-K\n",
    "        topk_scores, topk_indices = torch.topk(scores, self.k, dim=-1)\n",
    "\n",
    "        weights = F.softmax(topk_scores, dim=-1)\n",
    "\n",
    "        # IndexaÃ§Ã£o correta (SEM gather)\n",
    "        selected_memory = self.memory[topk_indices]  # [B, N, K, D]\n",
    "\n",
    "        retrieved = torch.sum(\n",
    "            weights.unsqueeze(-1) * selected_memory,\n",
    "            dim=2\n",
    "        )  # [B, N, D]\n",
    "\n",
    "        return x + retrieved\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Transformer Encoder Simples\n",
    "# ==========================================================\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, dim=128, heads=4, layers=2, use_engram=False):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=dim * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=layers\n",
    "        )\n",
    "\n",
    "        self.use_engram = use_engram\n",
    "\n",
    "        if use_engram:\n",
    "            self.engram = FastEngram(\n",
    "                memory_size=512,\n",
    "                dim=dim,\n",
    "                proj_dim=32,\n",
    "                k=16\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        if self.use_engram:\n",
    "            x = self.engram(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Teste de RAM\n",
    "# ==========================================================\n",
    "\n",
    "def test_memory(model, seq_len=128, dim=128, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    x = torch.randn(1, seq_len, dim).to(device)\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        tracemalloc.start()\n",
    "        _ = model(x)\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        return peak / (1024 * 1024)\n",
    "    else:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        _ = model(x)\n",
    "        peak = torch.cuda.max_memory_allocated(device)\n",
    "        return peak / (1024 * 1024)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Benchmark de Tempo\n",
    "# ==========================================================\n",
    "\n",
    "def benchmark(model, seq_len, dim=128, runs=20, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    x = torch.randn(1, seq_len, dim).to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        _ = model(x)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(runs):\n",
    "        _ = model(x)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    return (end - start) / runs\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Complexidade TeÃ³rica\n",
    "# ==========================================================\n",
    "\n",
    "def theoretical_complexity(seq_len,\n",
    "                           dim=128,\n",
    "                           memory_size=512,\n",
    "                           proj_dim=32,\n",
    "                           k=16):\n",
    "\n",
    "    # Transformer self-attention ~ O(nÂ²Â·d)\n",
    "    base = seq_len * seq_len * dim\n",
    "\n",
    "    # Engram otimizado\n",
    "    engram = (seq_len * memory_size * proj_dim) + \\\n",
    "             (seq_len * k * dim)\n",
    "\n",
    "    return base, base + engram\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    DIM = 128\n",
    "\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "\n",
    "    base_model = SimpleTransformer(dim=DIM, use_engram=False)\n",
    "    engram_model = SimpleTransformer(dim=DIM, use_engram=True)\n",
    "\n",
    "    # ================= RAM =================\n",
    "    print(\"\\n=== Teste de RAM ===\")\n",
    "\n",
    "    ram_base = test_memory(base_model, device=DEVICE)\n",
    "    ram_engram = test_memory(engram_model, device=DEVICE)\n",
    "\n",
    "    print(f\"RAM Base (MB): {ram_base:.2f}\")\n",
    "    print(f\"RAM Com Engram (MB): {ram_engram:.2f}\")\n",
    "\n",
    "    # ================= TEMPO =================\n",
    "    print(\"\\n=== Tempo vs seq_len ===\")\n",
    "\n",
    "    for seq in [16, 32, 64, 128]:\n",
    "\n",
    "        t_base = benchmark(base_model, seq, device=DEVICE)\n",
    "        t_engram = benchmark(engram_model, seq, device=DEVICE)\n",
    "\n",
    "        print(f\"\\nseq_len={seq}\")\n",
    "        print(f\"  Base:   {t_base:.6f}s\")\n",
    "        print(f\"  Engram: {t_engram:.6f}s\")\n",
    "\n",
    "    # ================= COMPLEXIDADE =================\n",
    "    print(\"\\n=== Complexidade TeÃ³rica ===\")\n",
    "\n",
    "    for seq in [16, 32, 64, 128]:\n",
    "\n",
    "        base_c, engram_c = theoretical_complexity(seq)\n",
    "\n",
    "        print(f\"\\nseq_len={seq}\")\n",
    "        print(f\"  Base complexity:   {base_c}\")\n",
    "        print(f\"  Engram complexity: {engram_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff0305",
   "metadata": {},
   "source": [
    "Os nÃºmeros mostram algo importante.\n",
    "\n",
    "## ğŸ” 1ï¸âƒ£ O tempo NÃƒO estÃ¡ escalando com `seq_len`\n",
    "\n",
    "Todos os tempos estÃ£o praticamente iguais:\n",
    "\n",
    "```\n",
    "Base   â‰ˆ 0.00085s\n",
    "Engram â‰ˆ 0.00101s\n",
    "```\n",
    "\n",
    "Para 16 â†’ 128 tokens isso deveria crescer ~4Ã— se o custo quadrÃ¡tico estivesse dominante.\n",
    "\n",
    "Isso significa:\n",
    "\n",
    "> ğŸ”¥ EstÃ¡ medindo overhead de kernel launch + sincronizaÃ§Ã£o, nÃ£o o custo real do modelo.\n",
    "\n",
    "Para sequÃªncias pequenas na GPU, o tempo Ã© dominado por:\n",
    "\n",
    "* LatÃªncia de launch\n",
    "* SincronizaÃ§Ã£o CUDA\n",
    "* Overhead do Python\n",
    "\n",
    "NÃ£o pelo FLOPs reais.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“ˆ 2ï¸âƒ£ RAM\n",
    "\n",
    "```\n",
    "Base:   12.82 MB\n",
    "Engram: 16.10 MB\n",
    "```\n",
    "\n",
    "DiferenÃ§a â‰ˆ 3.3 MB\n",
    "\n",
    "Isso Ã© **muito bom** â€” o Engram estÃ¡ leve.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“Š 3ï¸âƒ£ Complexidade\n",
    "\n",
    "Agora estÃ¡ correto matematicamente.\n",
    "\n",
    "Note que:\n",
    "\n",
    "Para seq_len = 128:\n",
    "\n",
    "Base:\n",
    "\n",
    "```\n",
    "2,097,152\n",
    "```\n",
    "\n",
    "Engram:\n",
    "\n",
    "```\n",
    "4,456,448\n",
    "```\n",
    "\n",
    "Ou seja:\n",
    "\n",
    "[\n",
    "Engram approx 2.1Ã— custo do Transformer\n",
    "]\n",
    "\n",
    "Mas isso sÃ³ aparece quando `n` fica grande.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš¨ O problema do benchmark\n",
    "\n",
    "O benchmark Ã© pequeno demais.\n",
    "\n",
    "Para ver o crescimento real, trocar:\n",
    "\n",
    "```python\n",
    "for seq in [16, 32, 64, 128]:\n",
    "```\n",
    "\n",
    "Por:\n",
    "\n",
    "```python\n",
    "for seq in [128, 256, 512, 1024]:\n",
    "```\n",
    "\n",
    "E aumente `runs` para 50.\n",
    "\n",
    "Vai comeÃ§ar a ver:\n",
    "\n",
    "* Base crescendo ~quadrÃ¡tico\n",
    "* Engram crescendo mais rÃ¡pido\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  Por que o tempo estÃ¡ estÃ¡vel?\n",
    "\n",
    "Porque:\n",
    "\n",
    "* GPU paraleliza tudo\n",
    "* M Ã© fixo (512)\n",
    "* proj_dim Ã© pequeno (32)\n",
    "* k Ã© pequeno (16)\n",
    "\n",
    "Logo o termo:\n",
    "\n",
    "[\n",
    "nÂ·mÂ·d'\n",
    "]\n",
    "\n",
    "Ainda nÃ£o Ã© dominante.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¬ Se vocÃª quiser forÃ§ar o Engram a pesar\n",
    "\n",
    "Mude:\n",
    "\n",
    "```python\n",
    "memory_size=4096\n",
    "proj_dim=64\n",
    "k=64\n",
    "```\n",
    "\n",
    "AÃ­ verÃ¡ o impacto real.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ ConclusÃ£o TÃ©cnica\n",
    "\n",
    "O Engram otimizado estÃ¡:\n",
    "\n",
    "âœ” EstÃ¡vel\n",
    "âœ” Com overhead pequeno\n",
    "âœ” EscalÃ¡vel\n",
    "âœ” Muito melhor que versÃ£o densa original\n",
    "\n",
    "Basicamente transformou:\n",
    "\n",
    "[\n",
    "O(nÂ·mÂ·d)\n",
    "]\n",
    "\n",
    "em algo muito mais controlÃ¡vel.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš€ PrÃ³ximo nÃ­vel (se quiser avanÃ§ar)\n",
    "\n",
    "Ã‰ possÃ­vel:\n",
    "\n",
    "* ğŸ”¥ Eliminar completamente o termo `nÂ·m` usando ANN real\n",
    "* ğŸ”¥ Transformar Engram em memÃ³ria hierÃ¡rquica\n",
    "* ğŸ”¥ Tornar custo quase linear em n\n",
    "* ğŸ”¥ Integrar FlashAttention\n",
    "* ğŸ”¥ Analisar limite prÃ¡tico para 4kâ€“8k tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de380b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
