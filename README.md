
# üìÑ Conditional Memory via Engram ‚Äî Explica√ß√£o do Artigo e Implementa√ß√£o

## üìÑ O que o artigo prop√µe?

O artigo <a href="https://arxiv.org/pdf/2601.07372">‚ÄúConditional Memory via Scalable Lookup‚Äù</a>, da DeepSeek-AI, prop√µe uma nova forma de aumentar a capacidade de modelos de linguagem.

A ideia central √© simples:

> Nem tudo precisa ser ‚Äúpensado‚Äù. Algumas coisas s√≥ precisam ser ‚Äúlembradas‚Äù.

Modelos atuais (como Transformers com MoE) usam computa√ß√£o para tudo ‚Äî inclusive para reconstruir padr√µes fixos como nomes pr√≥prios ou express√µes comuns.

Exemplo:

"Diana, Princess of Wales"

O modelo reconstr√≥i isso camada por camada, gastando profundidade da rede com algo que poderia ser simplesmente buscado.

---

## üß† O problema fundamental

Linguagem tem dois tipos de tarefa:

### 1Ô∏è‚É£ Racioc√≠nio din√¢mico
- Matem√°tica
- L√≥gica
- Cadeias longas de pensamento

‚Üí Precisa de computa√ß√£o.

### 2Ô∏è‚É£ Padr√µes est√°ticos
- Nomes
- Express√µes fixas
- Frases comuns

‚Üí Poderiam ser consultados como mem√≥ria.

Transformers n√£o t√™m um mecanismo nativo de ‚Äúlookup‚Äù (consulta direta).  
Eles simulam mem√≥ria usando computa√ß√£o.

Isso √© ineficiente.

---

## üí° A solu√ß√£o: Engram

O Engram √© um m√≥dulo de **mem√≥ria condicional**.

Ele adiciona ao modelo:

- Uma tabela enorme de vetores
- Um mecanismo de busca via N-grams
- Um sistema de ‚Äúgate‚Äù que decide quanto usar da mem√≥ria

Em vez de calcular tudo, o modelo pode consultar essa mem√≥ria.

---

## ‚öôÔ∏è Como o Engram funciona

### 1Ô∏è‚É£ O que √© ‚Äúcada posi√ß√£o‚Äù?

Quando uma frase entra no modelo, ela vira uma sequ√™ncia de tokens:

"Alexander the Great was king"

Pode virar algo como:

[1012, 45, 890, 77, 3001]

Cada token ocupa uma **posi√ß√£o** na sequ√™ncia.

Quando falamos ‚Äúpara cada posi√ß√£o‚Äù, significa:

> Para cada token da frase.

---

### 2Ô∏è‚É£ Busca na mem√≥ria (lookup)

Para cada posi√ß√£o, o Engram:

1. Pega os √∫ltimos N tokens (ex: 3-gram)
2. Aplica um hash determin√≠stico
3. Usa o resultado como √≠ndice
4. Busca um vetor em uma tabela enorme

Tabela de mem√≥ria:

```python
self.memory = nn.Embedding(table_size, memory_dim)
```

Se o hash retornar 5321:

```python
mem_vec = memory[5321]
```

Esse vetor representa um padr√£o lingu√≠stico aprendido.

---

### 3Ô∏è‚É£ O que √© hidden state?

O hidden state √© o vetor interno do Transformer em cada posi√ß√£o.

Ele cont√©m:

- Significado do token
- Contexto da frase
- Informa√ß√£o j√° processada

Formato t√≠pico:

(batch, seq_len, hidden_dim)

---

### 4Ô∏è‚É£ O que √© o gate?

O gate √© um n√∫mero entre 0 e 1 que decide:

- Quanto usar da mem√≥ria
- Ou ignor√°-la

F√≥rmula simplificada:

```python
alpha = sigmoid( dot(normalize(h), normalize(k)) / sqrt(d) )
```

Se combinarem bem ‚Üí alpha ‚âà 1  
Se n√£o combinarem ‚Üí alpha ‚âà 0  

Sa√≠da final da mem√≥ria:

saida_memoria = alpha * v

A mem√≥ria s√≥ √© usada se fizer sentido no contexto.

---

### 5Ô∏è‚É£ Inje√ß√£o via conex√£o residual

Depois que a mem√≥ria √© calculada, ela √© adicionada ao hidden state:

```python
novo_hidden = hidden + memoria
```

Isso √© conex√£o residual.

Ela:
- N√£o substitui o que o modelo j√° calculou
- Apenas adiciona informa√ß√£o

---

## üßÆ Fluxo completo

Para cada token:

1. Transformer calcula hidden_state
2. Engram pega √∫ltimos N tokens
3. Faz hash
4. Busca vetor na mem√≥ria
5. Calcula gate
6. Soma ao hidden_state

Resultado:

- üß† Computa√ß√£o din√¢mica
- üìö Mem√≥ria est√°tica r√°pida

---



---

## üöÄ Por que isso melhora o modelo?

O artigo mostra que:

- Modelos com Engram superam modelos apenas com MoE.
- Melhoram n√£o s√≥ mem√≥ria factual.
- Tamb√©m melhoram racioc√≠nio.

Porque o modelo para de gastar camadas reconstruindo padr√µes fixos.

Isso aumenta a **profundidade efetiva** da rede.

---

## üéØ Resumo final

O Engram separa dois mundos:

| Fun√ß√£o | Quem faz |
|--------|----------|
| Pensar | Transformer / MoE |
| Lembrar | Engram |

Essa separa√ß√£o:

- Melhora desempenho
- Escala melhor
- Reduz desperd√≠cio computacional
- Permite mem√≥ria gigantesca com baixo custo
