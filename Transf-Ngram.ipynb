{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96eb95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Teste de RAM ===\n",
      "RAM Base (MB): 17.47265625\n",
      "RAM Com Engram (MB): 29.578125\n",
      "\n",
      "=== Tempo vs seq_len ===\n",
      "seq_len=16\n",
      " Base: 0.000674s\n",
      " Engram: 0.000769s\n",
      "seq_len=32\n",
      " Base: 0.000685s\n",
      " Engram: 0.000974s\n",
      "seq_len=64\n",
      " Base: 0.001088s\n",
      " Engram: 0.001373s\n",
      "seq_len=128\n",
      " Base: 0.001862s\n",
      " Engram: 0.002382s\n",
      "\n",
      "=== Complexidade Te√≥rica ===\n",
      "seq_len=16\n",
      " Base complexity: 16384\n",
      " Engram complexity: 81920\n",
      "seq_len=32\n",
      " Base complexity: 65536\n",
      " Engram complexity: 196608\n",
      "seq_len=64\n",
      " Base complexity: 262144\n",
      " Engram complexity: 524288\n",
      "seq_len=128\n",
      " Base complexity: 1048576\n",
      " Engram complexity: 1572864\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# =========================\n",
    "# Modelo Base\n",
    "# =========================\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        x = self.transformer(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Engram Vetorizado\n",
    "# =========================\n",
    "class TinyEngram(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, table_size=5000, ngram_size=2):\n",
    "        super().__init__()\n",
    "        self.ngram_size = ngram_size\n",
    "        self.table_size = table_size\n",
    "\n",
    "        self.memory = nn.Embedding(table_size, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"hash_weights\",\n",
    "            torch.randint(1, 100, (ngram_size,))\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, hidden_states):\n",
    "        B, T = input_ids.shape\n",
    "\n",
    "        if T >= self.ngram_size:\n",
    "            ngrams = input_ids.unfold(1, self.ngram_size, 1)\n",
    "            pad = torch.zeros(\n",
    "                B,\n",
    "                self.ngram_size - 1,\n",
    "                self.ngram_size,\n",
    "                dtype=input_ids.dtype,\n",
    "                device=input_ids.device\n",
    "            )\n",
    "            ngrams = torch.cat([pad, ngrams], dim=1)\n",
    "        else:\n",
    "            ngrams = torch.zeros(\n",
    "                B,\n",
    "                T,\n",
    "                self.ngram_size,\n",
    "                dtype=input_ids.dtype,\n",
    "                device=input_ids.device\n",
    "            )\n",
    "\n",
    "        # Hash vetorizado simples\n",
    "        hash_vals = (ngrams * self.hash_weights).sum(-1) % self.table_size\n",
    "\n",
    "        mem_vec = self.memory(hash_vals)\n",
    "\n",
    "        k = self.key_proj(mem_vec)\n",
    "        v = self.value_proj(mem_vec)\n",
    "\n",
    "        h_norm = F.normalize(hidden_states, dim=-1)\n",
    "        k_norm = F.normalize(k, dim=-1)\n",
    "\n",
    "        alpha = torch.sigmoid(\n",
    "            (h_norm * k_norm).sum(-1, keepdim=True)\n",
    "            / math.sqrt(hidden_states.size(-1))\n",
    "        )\n",
    "\n",
    "        return hidden_states + alpha * v\n",
    "\n",
    "\n",
    "class TinyTransformerWithEngram(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "        self.engram = TinyEngram(hidden_dim=hidden_dim)\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        x = self.transformer(x)\n",
    "        x = self.engram(input_ids, x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark de Tempo\n",
    "# =========================\n",
    "def benchmark_time(model, input_ids, runs=10):\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(runs):\n",
    "            _ = model(input_ids)\n",
    "    return (time.time() - start) / runs\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Medi√ß√£o de RAM\n",
    "# =========================\n",
    "def measure_ram_mb():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Complexidade Te√≥rica\n",
    "# =========================\n",
    "def theoretical_transformer(seq_len, d):\n",
    "    # Aten√ß√£o dominante: O(T¬≤ * d)\n",
    "    return seq_len * seq_len * d\n",
    "\n",
    "def theoretical_engram(seq_len, d):\n",
    "    # Lookup + proje√ß√µes: O(T * d¬≤)\n",
    "    return seq_len * d * d\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exemplo de Uso\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    vocab_size = 2000\n",
    "    hidden_dim = 64\n",
    "    batch_size = 4\n",
    "\n",
    "    base_model = TinyTransformer(vocab_size, hidden_dim)\n",
    "    engram_model = TinyTransformerWithEngram(vocab_size, hidden_dim)\n",
    "\n",
    "    print(\"\\n=== Teste de RAM ===\")\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, 64))\n",
    "\n",
    "    ram_before = measure_ram_mb()\n",
    "    _ = base_model(input_ids)\n",
    "    ram_base = measure_ram_mb()\n",
    "\n",
    "    _ = engram_model(input_ids)\n",
    "    ram_engram = measure_ram_mb()\n",
    "\n",
    "    print(\"RAM Base (MB):\", ram_base - ram_before)\n",
    "    print(\"RAM Com Engram (MB):\", ram_engram - ram_before)\n",
    "\n",
    "    print(\"\\n=== Tempo vs seq_len ===\")\n",
    "    seq_lengths = [16, 32, 64, 128]\n",
    "\n",
    "    for sl in seq_lengths:\n",
    "        input_ids = torch.randint(0, vocab_size, (batch_size, sl))\n",
    "        t_base = benchmark_time(base_model, input_ids)\n",
    "        t_engram = benchmark_time(engram_model, input_ids)\n",
    "\n",
    "        print(f\"seq_len={sl}\")\n",
    "        print(f\" Base: {t_base:.6f}s\")\n",
    "        print(f\" Engram: {t_engram:.6f}s\")\n",
    "\n",
    "    print(\"\\n=== Complexidade Te√≥rica ===\")\n",
    "    for sl in seq_lengths:\n",
    "        c_base = theoretical_transformer(sl, hidden_dim)\n",
    "        c_engram = c_base + theoretical_engram(sl, hidden_dim)\n",
    "\n",
    "        print(f\"seq_len={sl}\")\n",
    "        print(f\" Base complexity: {c_base}\")\n",
    "        print(f\" Engram complexity: {c_engram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d3a45",
   "metadata": {},
   "source": [
    "\n",
    "## üìå 1Ô∏è‚É£ Uso de RAM\n",
    "\n",
    "* **Base:** 17.47 MB\n",
    "* **Com Engram:** 29.58 MB\n",
    "\n",
    "Diferen√ßa: **~12.1 MB a mais**\n",
    "Aumento relativo: **~69%**\n",
    "\n",
    "Isso indica que o m√≥dulo Engram adiciona uma estrutura de mem√≥ria significativa ‚Äî provavelmente buffers extras, cache de estados ou embeddings persistentes.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 2Ô∏è‚É£ Tempo de execu√ß√£o vs `seq_len`\n",
    "\n",
    "Os tempos crescem de forma consistente com o aumento da sequ√™ncia.\n",
    "\n",
    "### Base\n",
    "\n",
    "| seq_len | Tempo (s) |\n",
    "| ------- | --------- |\n",
    "| 16 | 0.000674 |\n",
    "| 32 | 0.000685 |\n",
    "| 64 | 0.001088 |\n",
    "| 128 | 0.001862 |\n",
    "\n",
    "### Engram\n",
    "\n",
    "| seq_len | Tempo (s) |\n",
    "| ------- | --------- |\n",
    "| 16 | 0.000769 |\n",
    "| 32 | 0.000974 |\n",
    "| 64 | 0.001373 |\n",
    "| 128 | 0.002382 |\n",
    "\n",
    "üìå Observa√ß√µes importantes:\n",
    "\n",
    "* O **Engram √© consistentemente mais lento**, mas n√£o dramaticamente.\n",
    "* O overhead aumenta conforme `seq_len` cresce.\n",
    "* A diferen√ßa absoluta √© pequena (fra√ß√µes de milissegundo).\n",
    "* A diferen√ßa relativa gira entre **15% e 30%** dependendo do tamanho.\n",
    "\n",
    "Isso sugere que o Engram adiciona custo proporcional ao tamanho da sequ√™ncia, mas n√£o altera drasticamente a ordem de crescimento.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 3Ô∏è‚É£ Complexidade Te√≥rica\n",
    "\n",
    "Base parece seguir:\n",
    "\n",
    "[\n",
    "O(n^2)\n",
    "]\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "* 16 ‚Üí 16384\n",
    "* 32 ‚Üí 65536 (4√ó)\n",
    "* 64 ‚Üí 262144 (4√ó)\n",
    "* 128 ‚Üí 1048576 (4√ó)\n",
    "\n",
    "Isso √© claramente crescimento quadr√°tico.\n",
    "\n",
    "---\n",
    "\n",
    "### Engram\n",
    "\n",
    "Valores:\n",
    "\n",
    "* 16 ‚Üí 81920\n",
    "* 32 ‚Üí 196608\n",
    "* 64 ‚Üí 524288\n",
    "* 128 ‚Üí 1572864\n",
    "\n",
    "O crescimento n√£o √© exatamente 4√ó a cada dobra. Parece algo como:\n",
    "\n",
    "[\n",
    "O(n^2 + n . m)\n",
    "]\n",
    "\n",
    "Ou seja, h√° um termo adicional linear multiplicado por algum fator fixo (provavelmente tamanho do banco de mem√≥ria).\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Conclus√£o T√©cnica\n",
    "\n",
    "‚úî O Engram aumenta:\n",
    "\n",
    "* Uso de RAM (~70%)\n",
    "* Tempo de execu√ß√£o (~15‚Äì30%)\n",
    "* Complexidade com termo adicional\n",
    "\n",
    "‚úî Mas:\n",
    "\n",
    "* N√£o muda a ordem dominante (continua aproximadamente quadr√°tico)\n",
    "* O overhead √© relativamente controlado\n",
    "* Escala de forma est√°vel\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Interpreta√ß√£o pr√°tica\n",
    "\n",
    "Se voc√™ estiver usando isso em:\n",
    "\n",
    "* üîπ Edge devices ‚Üí pode ser pesado\n",
    "* üîπ Servidor ‚Üí impacto pequeno\n",
    "* üîπ Treinamento grande escala ‚Üí pode virar gargalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd367b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
